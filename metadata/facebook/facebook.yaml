human_feature_desc:
  avg_bids_per_url: Average number of bids a user placed per referring URL
  balance: Balance measure based on the distribution of bids across different days
  bids_on_weekdays: Number of bids placed by the user on weekdays
  bids_per_auction_mean: Mean number of bids per auction for each bidder
  bids_per_auction_median: Median number of bids per auction for each bidder
  countries_per_bidder_per_auction_max: Maximum number of different countries per
    auction for each bidder
  countries_per_bidder_per_auction_mean: Mean number of different countries per auction
    for each bidder
  countries_per_bidder_per_auction_median: Median number of different countries per
    auction for each bidder
  day_entropy: the entropy for how many bids a user placed on each day of the week
  dt_change_ip_median: Median time between subsequent bids on different IPs for each
    user
  dt_others_median: Median time difference between a user's bid and the previous bid
    by another user in the same auction
  dt_others_min: Minimum time difference between a user's bid and the previous bid
    by another user in the same auction
  dt_same_ip_median: Median time between bids on the same IP for each user
  dt_self_median: Median time difference between consecutive bids made by the same
    user
  dt_self_min: Minimum time difference between consecutive bids made by the same user
  f_urls: Fraction of unique URLs over total bids by a bidder
  ip_cluster: Cluster label assigned to a bidder based on the set of IPs used (if
    clustering was performed)
  ip_entropy: Measure of diversity in IP addresses used by a bidder. Calculated using
    entropy
  ip_entropy_per_auction_mean: Mean of the IP entropy per auction for each user
  ip_only_one_user_counts: Count of bids from IPs that are only used by one user
  max_bids_in_20_min: Maximum number of bids placed by a user within any 20-minute
    span
  most_common_country: The most common country a bidder has placed bids from
  n_bids: Total number of bids made by a bidder
  n_bids_url: Total number of bids associated with a bidder's URL
  n_urls: Number of unique URLs used by a bidder
  num_first_bid: Number of times a bidder placed the first bid in an auction
  on_ip_that_has_a_bot: Indicator if a bidder has used an IP that has a bot on it
  on_ip_that_has_a_bot_mean: Mean of the indicator across different IPs used by the
    bidder
  sleep: Indicator of whether a bidder has a period of inactivity (sleep) based on
    bidding behavior
  t_since_start_median: Median time from the start of the auction until a bid
  t_until_end_median: Median time from a bid until the end of the auction
  url_entropy_per_auction_mean: Mean of the URL entropy per auction for each user
human_feature_impl:
  avg_bids_per_url: '

    # Compute the average number of bids per URL for each bidder

    avg_bids_per_url = Bids.groupby(''bidder_id'').apply(lambda x: x.groupby(''url'').size().mean()).reset_index()

    avg_bids_per_url.columns = [''bidder_id'', ''avg_bids_per_url'']


    # Merge the computed feature with the Bidders table

    Bidders = Bidders.merge(avg_bids_per_url, on=''bidder_id'', how=''left'')


    # Fill NaN values with 0 (in case some bidders have no bids)

    Bidders[''avg_bids_per_url''] = Bidders[''avg_bids_per_url'']


    # Display the updated Bidders table

    print(Bidders.head())

    '
  balance: '

    import pandas as pd


    # Assuming Bidders and Bids are already loaded as DataFrames

    # Bidders = pd.read_csv(''Bidders.csv'')

    # Bids = pd.read_csv(''Bids.csv'')


    # Convert the time column to datetime

    Bids[''time''] = pd.to_datetime(Bids[''time''])


    # Extract the day from the time column

    Bids[''day''] = Bids[''time''].dt.date


    # Calculate the number of bids per day for each bidder

    bids_per_day = Bids.groupby([''bidder_id'', ''day'']).size().reset_index(name=''bids_per_day'')


    # Calculate the balance measure for each bidder

    balance = bids_per_day.groupby(''bidder_id'')[''bids_per_day''].apply(lambda x:
    x.std() / x.mean() if x.mean() != 0 else 0).reset_index(name=''balance'')


    # Merge the balance feature into the Bidders table

    Bidders = Bidders.merge(balance, on=''bidder_id'', how=''left'')


    # Fill NaN values with 0 (in case some bidders have no bids)

    Bidders[''balance''] = Bidders[''balance'']


    # Display the updated Bidders table

    print(Bidders.head())

    '
  bids_on_weekdays: '

    import pandas as pd


    # Convert the ''time'' column to datetime

    Bids[''time''] = pd.to_datetime(Bids[''time''])


    # Extract the day of the week from the ''time'' column (0=Monday, 6=Sunday)

    Bids[''weekday''] = Bids[''time''].dt.weekday


    # Filter bids that were placed on weekdays (0-4)

    weekday_bids = Bids[Bids[''weekday''] < 5]


    # Count the number of weekday bids per bidder

    weekday_bids_count = weekday_bids.groupby(''bidder_id'').size().reset_index(name=''bids_on_weekdays'')


    # Merge the count of weekday bids with the Bidders table

    Bidders = Bidders.merge(weekday_bids_count, on=''bidder_id'', how=''left'')


    # Fill NaN values with 0 (for bidders with no weekday bids)

    Bidders[''bids_on_weekdays''] = Bidders[''bids_on_weekdays'']


    # Display the updated Bidders table

    print(Bidders)

    '
  bids_per_auction_mean: '

    bids_per_auction = Bids.groupby(''bidder_id'')[''auction''].nunique()

    bids_per_auction_mean = Bids.groupby(''bidder_id'').size() / bids_per_auction

    Bidders = Bidders.set_index(''bidder_id'')

    Bidders[''bids_per_auction_mean''] = bids_per_auction_mean

    Bidders = Bidders.reset_index()

    '
  bids_per_auction_median: '

    bids_per_auction = Bids.groupby([''bidder_id'', ''auction'']).size().reset_index(name=''bids_per_auction'')

    bids_per_auction_median = bids_per_auction.groupby(''bidder_id'')[''bids_per_auction''].median().reset_index(name=''bids_per_auction_median'')

    Bidders = Bidders.merge(bids_per_auction_median, on=''bidder_id'', how=''left'')

    '
  countries_per_bidder_per_auction_max: '

    # Group by bidder_id and auction, then count unique countries per auction

    countries_per_auction = Bids.groupby([''bidder_id'', ''auction''])[''country''].nunique().reset_index()


    # Find the maximum number of different countries per auction for each bidder

    max_countries_per_bidder = countries_per_auction.groupby(''bidder_id'')[''country''].max().reset_index()


    # Rename the columns to match the desired feature name

    max_countries_per_bidder.columns = [''bidder_id'', ''countries_per_bidder_per_auction_max'']


    # Merge the new feature into the Bidders table

    Bidders = Bidders.merge(max_countries_per_bidder, on=''bidder_id'', how=''left'')


    # Fill NaN values with 0 (assuming that bidders with no bids should have 0 as
    the feature value)

    Bidders[''countries_per_bidder_per_auction_max'']

    '
  countries_per_bidder_per_auction_mean: '

    # Group by bidder_id and auction, then count unique countries per group

    countries_per_auction = Bids.groupby([''bidder_id'', ''auction''])[''country''].nunique().reset_index()


    # Group by bidder_id to calculate the mean number of different countries per auction

    countries_per_bidder_per_auction_mean = countries_per_auction.groupby(''bidder_id'')[''country''].mean().reset_index()


    # Rename the columns for clarity

    countries_per_bidder_per_auction_mean.columns = [''bidder_id'', ''countries_per_bidder_per_auction_mean'']


    # Merge the new feature into the Bidders table

    Bidders = Bidders.merge(countries_per_bidder_per_auction_mean, on=''bidder_id'',
    how=''left'')


    # Fill NaN values with 0 (if any bidder_id in Bidders does not have corresponding
    bids)

    Bidders[''countries_per_bidder_per_auction_mean'']

    '
  countries_per_bidder_per_auction_median: '

    # Compute the median number of different countries per auction for each bidder

    countries_per_auction = Bids.groupby([''bidder_id'', ''auction''])[''country''].nunique().reset_index(name=''countries_per_auction'')

    median_countries_per_bidder = countries_per_auction.groupby(''bidder_id'')[''countries_per_auction''].median().reset_index(name=''countries_per_bidder_per_auction_median'')


    # Merge the computed feature with the Bidders table

    Bidders = Bidders.merge(median_countries_per_bidder, on=''bidder_id'', how=''left'')


    # Fill NaN values with 0 (or any other appropriate value)

    Bidders[''countries_per_bidder_per_auction_median'']

    '
  day_entropy: '

    import pandas as pd

    import numpy as np

    from scipy.stats import entropy


    # Convert time to datetime to extract day of the week

    Bids[''time''] = pd.to_datetime(Bids[''time''], unit=''s'')

    Bids[''day_of_week''] = Bids[''time''].dt.dayofweek


    day_entropy = Bids.groupby(''bidder_id'')[''day_of_week''].agg(lambda x: entropy(x.value_counts()))

    day_entropy.name = ''day_entropy''

    Bidders = Bidders.merge(day_entropy, on=''bidder_id'', how=''left'')

    '
  dt_change_ip_median: '

    import pandas as pd


    # Ensure the time column is in datetime format

    Bids[''time''] = pd.to_datetime(Bids[''time''])


    # Sort bids by bidder_id and time

    Bids = Bids.sort_values(by=[''bidder_id'', ''time''])


    # Calculate the time difference between subsequent bids

    Bids[''time_diff''] = Bids.groupby(''bidder_id'')[''time''].diff()


    # Identify changes in IP

    Bids[''ip_change''] = Bids.groupby(''bidder_id'')[''ip''].shift() != Bids[''ip'']


    # Filter time differences where IP changes

    ip_change_times = Bids[Bids[''ip_change''] == True].groupby(''bidder_id'')[''time_diff''].median()


    # Merge the median time differences back to the Bidders table

    Bidders = Bidders.merge(ip_change_times.rename(''dt_change_ip_median''), on=''bidder_id'',
    how=''left'')


    # Fill NaN values with 0 (or any other appropriate value)

    Bidders[''dt_change_ip_median''] = Bidders[''dt_change_ip_median'']


    # Display the updated Bidders table

    print(Bidders.head())

    '
  dt_others_median: '

    import pandas as pd


    # Convert time to datetime for easier manipulation

    Bids[''time''] = pd.to_datetime(Bids[''time''])


    # Sort bids by auction and time

    Bids = Bids.sort_values(by=[''auction'', ''time''])


    # Compute time differences within each auction

    Bids[''time_diff''] = Bids.groupby(''auction'')[''time''].diff()


    # Filter out the time differences that are not between different users

    Bids[''prev_bidder''] = Bids.groupby(''auction'')[''bidder_id''].shift()

    Bids[''is_diff_user''] = Bids[''bidder_id''] != Bids[''prev_bidder'']

    Bids_diff_user = Bids[Bids[''is_diff_user'']]


    # Compute the median time difference for each bidder

    median_time_diff = Bids_diff_user.groupby(''bidder_id'')[''time_diff''].median().reset_index()

    median_time_diff.columns = [''bidder_id'', ''dt_others_median'']


    # Merge the median time difference back to the Bidders table

    Bidders = Bidders.merge(median_time_diff, on=''bidder_id'', how=''left'')


    # Fill NaN values with 0 (or any other appropriate value)

    Bidders[''dt_others_median''] = Bidders[''dt_others_median'']


    # Convert the timedelta to seconds for easier interpretation

    Bidders[''dt_others_median''] = Bidders[''dt_others_median''].dt.total_seconds()


    Bidders.head()

    '
  dt_others_min: '

    import pandas as pd


    # Convert time to datetime for easier manipulation

    Bids[''time''] = pd.to_datetime(Bids[''time''])


    # Sort bids by auction and time

    Bids = Bids.sort_values(by=[''auction'', ''time''])


    # Compute the time difference between consecutive bids in the same auction

    Bids[''time_diff''] = Bids.groupby(''auction'')[''time''].diff()


    # Create a mask to identify bids that are not from the same bidder as the previous
    bid

    mask = Bids[''bidder_id''] != Bids[''bidder_id''].shift()


    # Apply the mask to keep only the time differences where the previous bid was
    from a different bidder

    Bids[''dt_others''] = Bids[''time_diff''].where(mask)


    # Group by bidder_id and find the minimum time difference for each bidder

    min_time_diff = Bids.groupby(''bidder_id'')[''dt_others''].min().reset_index()


    # Rename the column to dt_others_min

    min_time_diff = min_time_diff.rename(columns={''dt_others'': ''dt_others_min''})


    # Merge the new feature into the Bidders table

    Bidders = Bidders.merge(min_time_diff, on=''bidder_id'', how=''left'')


    # Fill NaN values with a large number (or any other strategy you prefer)

    Bidders[''dt_others_min''] = Bidders[''dt_others_min'']


    # Convert the timedelta to seconds for easier interpretation

    Bidders[''dt_others_min''] = Bidders[''dt_others_min''].dt.total_seconds()


    # Display the updated Bidders table

    print(Bidders.head())

    '
  dt_same_ip_median: '

    import pandas as pd


    # Assuming Bidders and Bids DataFrames are already loaded

    # Bidders = pd.read_csv(''Bidders.csv'')

    # Bids = pd.read_csv(''Bids.csv'')


    # Calculate the median time between bids on the same IP for each user

    Bids[''time_diff''] = Bids.groupby([''bidder_id'', ''ip''])[''time''].diff()

    dt_same_ip_median = Bids.groupby(''bidder_id'')[''time_diff''].median()


    # Add the feature to the Bidders table

    Bidders = Bidders.set_index(''bidder_id'')

    Bidders[''dt_same_ip_median''] = dt_same_ip_median

    Bidders = Bidders.reset_index()


    # Fill NaN values with 0 (or any other appropriate value)

    Bidders[''dt_same_ip_median''] = Bidders[''dt_same_ip_median'']


    # Display the updated Bidders table

    print(Bidders.head())

    '
  dt_self_median: '

    import pandas as pd


    # Assuming Bidders and Bids are already loaded as DataFrames


    # Compute the median time difference between consecutive bids made by the same
    user

    Bids = Bids.sort_values(by=[''bidder_id'', ''time''])

    Bids[''time_diff''] = Bids.groupby(''bidder_id'')[''time''].diff()

    median_time_diff = Bids.groupby(''bidder_id'')[''time_diff''].median().reset_index()

    median_time_diff.columns = [''bidder_id'', ''dt_self_median'']


    # Merge the new feature into the Bidders table

    Bidders = Bidders.merge(median_time_diff, on=''bidder_id'', how=''left'')


    # Fill NaN values with 0 (or any other appropriate value)

    Bidders[''dt_self_median''] = Bidders[''dt_self_median'']


    # Display the updated Bidders table

    print(Bidders.head())

    '
  dt_self_min: '

    import pandas as pd


    # Assuming Bidders and Bids are already loaded as DataFrames


    # Convert time to datetime if it''s not already

    Bids[''time''] = pd.to_datetime(Bids[''time''])


    # Sort bids by bidder_id and time

    Bids = Bids.sort_values(by=[''bidder_id'', ''time''])


    # Calculate the time difference between consecutive bids for each bidder

    Bids[''time_diff''] = Bids.groupby(''bidder_id'')[''time''].diff()


    # Get the minimum time difference for each bidder

    min_time_diff = Bids.groupby(''bidder_id'')[''time_diff''].min().reset_index()


    # Rename the columns for merging

    min_time_diff.columns = [''bidder_id'', ''dt_self_min'']


    # Merge the minimum time difference back to the Bidders table

    Bidders = Bidders.merge(min_time_diff, on=''bidder_id'', how=''left'')


    # Fill NaN values with a large number (or any other strategy you prefer)

    Bidders[''dt_self_min''] = Bidders[''dt_self_min'']


    # Convert the time difference to seconds for easier interpretation

    Bidders[''dt_self_min''] = Bidders[''dt_self_min''].dt.total_seconds()


    # Display the updated Bidders table

    print(Bidders.head())

    '
  f_urls: '

    # Group by bidder_id and calculate the fraction of unique URLs over total bids

    url_fraction = Bids.groupby(''bidder_id'').apply(lambda x: x[''url''].nunique()
    / len(x)).reset_index(name=''f_urls'')


    # Merge the calculated feature with the Bidders table

    Bidders = Bidders.merge(url_fraction, on=''bidder_id'', how=''left'')


    # Fill NaN values with 0 (in case some bidders have no bids)

    Bidders[''f_urls''] = Bidders[''f_urls'']

    '
  ip_cluster: '

    from sklearn.cluster import KMeans

    import numpy as np


    # Aggregate IPs for each bidder

    bidder_ips = Bids.groupby(''bidder_id'')[''ip''].apply(list).reset_index()


    # Convert list of IPs to a string to use as a feature for clustering

    bidder_ips[''ip_str''] = bidder_ips[''ip''].apply(lambda x: '' ''.join(map(str,
    x)))


    # Vectorize the IP strings

    from sklearn.feature_extraction.text import TfidfVectorizer

    vectorizer = TfidfVectorizer()

    ip_vectors = vectorizer.fit_transform(bidder_ips[''ip_str''])


    # Perform clustering

    kmeans = KMeans(n_clusters=10, random_state=0).fit(ip_vectors)

    bidder_ips[''ip_cluster''] = kmeans.labels_


    # Merge the cluster labels back to the Bidders table

    Bidders = Bidders.merge(bidder_ips[[''bidder_id'', ''ip_cluster'']], on=''bidder_id'',
    how=''left'')


    # Fill NaN values with a default cluster (e.g., -1)

    Bidders[''ip_cluster'']

    '
  ip_entropy: '

    import pandas as pd

    from scipy.stats import entropy


    # Assuming Bidders and Bids DataFrames are already loaded


    ip_entropy = Bids.groupby(''bidder_id'')[''ip''].agg(lambda x: entropy(x.value_counts()))

    ip_entropy.name = ''ip_entropy''

    Bidders = Bidders.merge(ip_entropy, on=''bidder_id'', how=''left'')

    '
  ip_entropy_per_auction_mean: "\nimport pandas as pd\nfrom scipy.stats import entropy\n\
    \n# Assuming Bidders and Bids are already loaded as DataFrames\n# Bidders = pd.read_csv('Bidders.csv')\n\
    # Bids = pd.read_csv('Bids.csv')\n\n# Calculate IP entropy per auction\ndef calculate_ip_entropy(auction_df):\n\
    \    ip_counts = auction_df['ip'].value_counts()\n    return entropy(ip_counts)\n\
    \n# Group by bidder and auction, then calculate the IP entropy for each auction\n\
    auction_ip_entropy = Bids.groupby(['bidder_id', 'auction']).apply(lambda x: calculate_ip_entropy(x)).reset_index(name='ip_entropy')\n\
    \n# Calculate the mean IP entropy per auction for each bidder\nmean_ip_entropy_per_bidder\
    \ = auction_ip_entropy.groupby('bidder_id')['ip_entropy'].mean().reset_index(name='ip_entropy_per_auction_mean')\n\
    \n# Merge the new feature into the Bidders table\nBidders = Bidders.merge(mean_ip_entropy_per_bidder,\
    \ on='bidder_id', how='left')\n\n# Fill NaN values with 0 (or any other appropriate\
    \ value)\nBidders['ip_entropy_per_auction_mean']\n"
  ip_only_one_user_counts: '

    import pandas as pd


    # Assuming Bidders and Bids DataFrames are already loaded


    # Count the number of unique bidders per IP

    ip_bidder_counts = Bids.groupby(''ip'')[''bidder_id''].nunique()


    # Filter IPs that are used by only one bidder

    ips_with_one_user = ip_bidder_counts[ip_bidder_counts == 1].index


    # Count the number of bids from these IPs for each bidder

    bids_from_ips_with_one_user = Bids[Bids[''ip''].isin(ips_with_one_user)]

    ip_only_one_user_counts = bids_from_ips_with_one_user.groupby(''bidder_id'').size()


    # Add the new feature to the Bidders table

    Bidders = Bidders.set_index(''bidder_id'')

    Bidders[''ip_only_one_user_counts''] = ip_only_one_user_counts

    Bidders = Bidders.reset_index()


    # Display the updated Bidders table

    print(Bidders.head())

    '
  max_bids_in_20_min: "\nimport pandas as pd\n\n# Assuming Bidders and Bids DataFrames\
    \ are already loaded\n\n# Convert time to datetime for easier manipulation\nBids['time']\
    \ = pd.to_datetime(Bids['time'])\n\n# Sort bids by bidder_id and time\nBids =\
    \ Bids.sort_values(by=['bidder_id', 'time'])\n\n# Function to calculate max bids\
    \ in any 20-minute span for a given bidder\ndef max_bids_in_20_min(bidder_bids):\n\
    \    max_bids = 0\n    for i in range(len(bidder_bids)):\n        start_time =\
    \ bidder_bids.iloc[i]['time']\n        end_time = start_time + pd.Timedelta(minutes=20)\n\
    \        bids_in_span = bidder_bids[(bidder_bids['time'] >= start_time) & (bidder_bids['time']\
    \ <= end_time)]\n        max_bids = max(max_bids, len(bids_in_span))\n    return\
    \ max_bids\n\n# Group bids by bidder_id and apply the function\nmax_bids_per_bidder\
    \ = Bids.groupby('bidder_id').apply(lambda x: max_bids_in_20_min(x)).reset_index()\n\
    max_bids_per_bidder.columns = ['bidder_id', 'max_bids_in_20_min']\n\n# Merge the\
    \ new feature into the Bidders table\nBidders = Bidders.merge(max_bids_per_bidder,\
    \ on='bidder_id', how='left')\n\n# Fill NaN values with 0 (in case some bidders\
    \ have no bids)\nBidders['max_bids_in_20_min'] = Bidders['max_bids_in_20_min']\n\
    \n# Display the updated Bidders table\nprint(Bidders.head())\n"
  most_common_country: "\ndef find_most_common_index(x):\n    c = x.value_counts()\n\
    \    if len(c) == 0:\n        return None\n    else:\n        return c.idxmax()\n\
    \n# Compute the most common country a bidder has placed bids from\nmost_common_country\
    \ = Bids.groupby('bidder_id')['country'].agg(find_most_common_index).reset_index()\n\
    \n# Rename the column to 'most_common_country'\nmost_common_country.columns =\
    \ ['bidder_id', 'most_common_country']\n\n# Merge the new feature into the Bidders\
    \ table\nBidders = Bidders.merge(most_common_country, on='bidder_id', how='left')\n\
    \n# Display the updated Bidders table\nprint(Bidders.head())\n"
  n_bids: '

    n_bids = Bids.groupby(''bidder_id'').size().reset_index(name=''n_bids'')

    Bidders = Bidders.merge(n_bids, on=''bidder_id'', how=''left'')

    '
  n_bids_url: '

    n_bids_url = Bids.groupby(''bidder_id'')[''url''].count().reset_index(name=''n_bids_url'')

    Bidders = Bidders.merge(n_bids_url, on=''bidder_id'', how=''left'')

    '
  n_urls: '

    n_urls = Bids.groupby(''bidder_id'')[''url''].nunique().reset_index()

    n_urls.columns = [''bidder_id'', ''n_urls'']

    Bidders = Bidders.merge(n_urls, on=''bidder_id'', how=''left'')

    Bidders[''n_urls''] = Bidders[''n_urls'']

    '
  num_first_bid: '

    first_bids = Bids.sort_values(by=[''auction'', ''time'']).drop_duplicates(subset=[''auction''],
    keep=''first'')

    num_first_bid = first_bids[''bidder_id''].value_counts().reset_index()

    num_first_bid.columns = [''bidder_id'', ''num_first_bid'']

    Bidders = Bidders.merge(num_first_bid, on=''bidder_id'', how=''left'')

    '
  on_ip_that_has_a_bot: '

    # Identify IPs that have been used by bots

    bot_ips = Bids[Bids[''bidder_id''].isin(Bidders[Bidders[''outcome''] == 1.0][''bidder_id''])][''ip''].unique()


    # Create a new feature indicating if a bidder has used an IP that has a bot on
    it

    Bidders[''on_ip_that_has_a_bot''] = Bidders[''bidder_id''].isin(Bids[Bids[''ip''].isin(bot_ips)][''bidder_id'']).astype(int)

    '
  on_ip_that_has_a_bot_mean: '

    # Merge Bidders with Bids to get the outcome for each bid

    merged_df = Bids.merge(Bidders[[''bidder_id'', ''outcome'']], on=''bidder_id'',
    how=''left'')


    # Group by IP and calculate the mean outcome for each IP

    ip_bot_mean = merged_df.groupby(''ip'')[''outcome''].mean().reset_index()

    ip_bot_mean.columns = [''ip'', ''on_ip_that_has_a_bot_mean'']


    # Merge the mean outcome back to the Bids dataframe

    merged_df = merged_df.merge(ip_bot_mean, on=''ip'', how=''left'')


    # Group by bidder_id and calculate the mean of ''on_ip_that_has_a_bot_mean''

    bidder_ip_bot_mean = merged_df.groupby(''bidder_id'')[''on_ip_that_has_a_bot_mean''].mean().reset_index()

    bidder_ip_bot_mean.columns = [''bidder_id'', ''on_ip_that_has_a_bot_mean'']


    # Merge the result back to the Bidders dataframe

    Bidders = Bidders.merge(bidder_ip_bot_mean, on=''bidder_id'', how=''left'')


    # Fill NaN values with 0 (if any)

    Bidders[''on_ip_that_has_a_bot_mean'']

    '
  sleep: '

    import pandas as pd


    # Assuming Bidders and Bids DataFrames are already loaded


    # Convert time to datetime for easier manipulation

    Bids[''time''] = pd.to_datetime(Bids[''time''])


    # Define a threshold for inactivity (e.g., 1 day)

    inactivity_threshold = pd.Timedelta(days=1)


    # Calculate the difference in time between consecutive bids for each bidder

    Bids = Bids.sort_values(by=[''bidder_id'', ''time''])

    Bids[''time_diff''] = Bids.groupby(''bidder_id'')[''time''].diff()


    # Determine if there is any period of inactivity greater than the threshold

    Bids[''sleep''] = Bids[''time_diff''] > inactivity_threshold


    # Aggregate to find if any sleep period exists for each bidder

    sleep_feature = Bids.groupby(''bidder_id'')[''sleep''].any().reset_index()


    # Merge the sleep feature back to the Bidders table

    Bidders = Bidders.merge(sleep_feature, on=''bidder_id'', how=''left'')

    '
  t_since_start_median: '

    import pandas as pd


    # Assuming Bidders and Bids are already loaded as DataFrames


    # Compute the median time from the start of the auction until a bid for each bidder

    t_since_start_median = Bids.groupby(''bidder_id'')[''time''].median().reset_index()

    t_since_start_median.columns = [''bidder_id'', ''t_since_start_median'']


    # Merge the computed feature with the Bidders table

    Bidders = Bidders.merge(t_since_start_median, on=''bidder_id'', how=''left'')


    # Fill NaN values with a default value (e.g., 0) if necessary

    Bidders[''t_since_start_median'']

    '
  t_until_end_median: '

    import pandas as pd


    # Assuming Bidders and Bids are already loaded as DataFrames


    # Compute the median time from a bid until the end of the auction

    Bids[''time_until_end''] = Bids.groupby(''auction'')[''time''].transform(lambda
    x: x.max() - x)

    median_time_until_end = Bids.groupby(''bidder_id'')[''time_until_end''].median().reset_index()


    # Merge the computed feature with the Bidders table

    Bidders = Bidders.merge(median_time_until_end, on=''bidder_id'', how=''left'')

    Bidders.rename(columns={''time_until_end'': ''t_until_end_median''}, inplace=True)

    '
  url_entropy_per_auction_mean: "\nimport pandas as pd\nfrom scipy.stats import entropy\n\
    \n# Function to calculate entropy of URLs\ndef calculate_url_entropy(urls):\n\
    \    value_counts = pd.Series(urls).value_counts()\n    return entropy(value_counts)\n\
    \n# Calculate URL entropy per auction\nurl_entropy_per_auction = Bids.groupby(['auction',\
    \ 'bidder_id'])['url'].apply(calculate_url_entropy).reset_index(name='url_entropy')\n\
    \n# Calculate mean URL entropy per auction for each user\nurl_entropy_per_auction_mean\
    \ = url_entropy_per_auction.groupby('bidder_id')['url_entropy'].mean().reset_index(name='url_entropy_per_auction_mean')\n\
    \n# Merge the new feature into the Bidders table\nBidders = Bidders.merge(url_entropy_per_auction_mean,\
    \ on='bidder_id', how='left')\n\n# Fill NaN values with 0 (if any bidder_id in\
    \ Bidders does not have corresponding bids)\nBidders['url_entropy_per_auction_mean']\n"
name: facebook
table_path: data/facebook
table_schemas:
- columns:
  - description: Unique identifier of a bidder.
    dtype: primary_key
    name: bidder_id
  - description: 'Label of a bidder indicating whether or not it is a robot. Value
      1.0 indicates a robot, where value 0.0 indicates human. The outcome was half
      hand labeled, half stats-based. There are two types of ''bots'' with different
      levels of proof: 1. Bidders who are identified as bots/fraudulent with clear
      proof. Their accounts were banned by the auction site. 2. Bidder who may have
      just started their business/clicks or their stats exceed from system wide average.
      There are no clear proof that they are bots.'
    dtype: bool
    name: outcome
  name: Bidders
  time_column: null
- columns:
  - description: Unique id for this bid.
    dtype: primary_key
    name: bid_id
  - description: Unique identifier of a bidder (same as the bidder_id used in train.csv
      and test.csv).
    dtype: foreign_key
    link_to: Bidders.bidder_id
    name: bidder_id
  - description: Unique identifier of an auction.
    dtype: category
    name: auction
  - description: The category of the auction site campaign, which means the bidder
      might come to this site by way of searching for 'home goods' but ended up bidding
      for 'sporting goods' - and that leads to this field being 'home goods'. This
      categorical field could be a search term, or online advertisement.
    dtype: category
    name: merchandise
  - description: Phone model of a visitor.
    dtype: category
    name: device
  - description: Time that the bid is made (transformed to protect privacy).
    dtype: datetime
    name: time
  - description: The country that the IP belongs to.
    dtype: category
    name: country
  - description: IP address of a bidder (obfuscated to protect privacy).
    dtype: text
    name: ip
  - description: URL where the bidder was referred from (obfuscated to protect privacy).
    dtype: category
    name: url
  name: Bids
  time_column: null
target_column: outcome
target_table: Bidders
task_split: null
