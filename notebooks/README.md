LLMs in general have a hard time dealing with extremely long context, so
I want to make the prompt as self-contained and as simple as possible,
especially since we are just making a quick baseline.

I divided the feature engineering process into two stages:

1. Feed in the description of the dataset, and invoke the LLM to propose a
   list of new features using natural language.
2. For each new feature proposed, invoke the LLM with the description of the
   dataset and the description of the new feature to generate pandas code.
   If N features are proposed, the LLM will be invoked N times, once per new
   feature.

TODO:

One problem is that Claude 3, LLaMA 3 and Mixtral sometimes ignore more
detailed instructions such as "please add a new feature called 'NewFeature'",
"please do not generate any explanation", etc.  So we need to programmatically
deal with that during in our evaluation.  Luckily, so far all LLMs wrap their
code within a Markdown code block (triple backquotes), so finding the actual
Python code block among the answer is easy.  To find which columns are
generated by the LLM, we need to execute the code and find the difference
of the set of columns between the result and the original table.
