# LLM_datascience

This repository contains projects and experiments related to the paper: **"ELF-Gym: Evaluating Large Language Models Generated Features for Tabular Prediction."**

## Running the Experiments
### Download the Dataset

To download the dataset, you can go to kaggle to find the competition and download the origin dataset.

In our experiments, we alter the table_name of the origin dataset to match the table_name in the metadata file. You should run the preprocess code in `data` folder first.

### Execute Experiments

To run experiments, execute the following command:

```bash
bash run.sh
```

This code includes two steps: the first step is to generate features using the llm. The second step is to evaluate the generated features.

To complete the first step, you can run:
```bash
python main.py generate $task_path $llm $llm_output_path
```
where `$task_path` is the path to the metadata configuration file path, `$llm` is the name of the llm, and `$llm_output_path` is the output path for the features generated by the llm. 

For more usage information about this command, please enter `python main.py generate --help`.

To complete the second step, you can run:
```bash
python main.py evaluate $task_path $llm_output_path $evaluation_name $output_path
```
where `$evaluation_name` is the name of the evaluation method, which includes semantic|functional|correlation|downstream. `$output_path` is the output path for the evaluation results. 

For more usage information about this command, please enter `python main.py evaluate --help`.

### To get the number in paper
Due to the randomness in the results generated by llm, the features you generate locally may differ from those we produced at the time. Therefore, we have provided the features generated by llm during our experiments. These features are stored in the `./features_llm/$dataname` folder.

So if you want to get the number in the paper, you can set the `$llm_output_path` as `./features_llm/$dataname`.

### How to get the prompt

The prompts are stored in the `tools/templates` folder.

### To run on your own dataset

To utilize your own dataset, please follow these steps:

1. **Add Dataset**: Place your dataset in the data folder.
2. **Add Metadata Configuration**: Provide the metadata for your dataset, which should include `human_feature_description`, `human_feature_implementation_code`, `dataset_name`, `table_path`, and `table_schemas`, as demonstrated in the `metadata` folder.

## Cite
```bibtex
@inproceedings{zhang2024elf,
  author    = {Zhang, Yanlin and Li, Ning and Gan, Quan and Zhang, Weinan and Wipf, David and Wang, Minjie},
  title     = {ELF-Gym: Evaluating Large Language Models Generated Features for Tabular Prediction.},
  year      = {2024},
  booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM '24), October 21--25, 2024, Boise, ID, USA}
}
```

## Acknowledgements
This project was completed during my internship at AWS. 

Special thanks to [jermainewang](https://github.com/jermainewang), [BarclayII](https://github.com/BarclayII), [NingLi670](https://github.com/NingLi670), David and Weinan Zhang for their assistance!!!
